{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! CMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmyLF_uA9xAI",
        "outputId": "579878fb-404e-4c1b-f61b-dec5e7d1a54e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download winterForestStump/Phi-3.5-instruct-CBR_Bonds_info unsloth.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False"
      ],
      "metadata": {
        "id": "LFSdx4MY9VFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ce9e54-e1c6-4a9d-d914-6c132c1d482f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:139: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'unsloth.Q4_K_M.gguf' to 'models/.cache/huggingface/download/unsloth.Q4_K_M.gguf.846fc484a9d95cb2e1454852c172c4d985bfa9624c331d164986daf75ea10bd4.incomplete'\n",
            "unsloth.Q4_K_M.gguf: 100% 2.32G/2.32G [00:54<00:00, 42.2MB/s]\n",
            "Download complete. Moving file to models/unsloth.Q4_K_M.gguf\n",
            "models/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "6_90PfwWcaJ-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama(\n",
        "      model_path=\"/content/models/unsloth.Q4_K_M.gguf\",\n",
        "      # n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
        "      seed=1337, # Uncomment to set a specific seed\n",
        "      n_ctx=2048, # Uncomment to increase the context window\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCvnoYNpcaz_",
        "outputId": "9f07fcaa-338a-4a02-9e4f-9aea2f01bd7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 34 key-value pairs and 291 tensors from /content/models/unsloth.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = phi-3.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 96\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 96\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 32064\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 32009\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: control token:  32008 '<|placeholder5|>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32006 '<|system|>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32002 '<|placeholder1|>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32001 '<|assistant|>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32004 '<|placeholder3|>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32003 '<|placeholder2|>' is not marked as EOG\n",
            "llm_load_vocab: control token:      0 '<unk>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32005 '<|placeholder4|>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32010 '<|user|>' is not marked as EOG\n",
            "llm_load_vocab: control token:  32009 '<|placeholder6|>' is not marked as EOG\n",
            "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
            "llm_load_vocab: special tokens cache size = 14\n",
            "llm_load_vocab: token to piece cache size = 0.1685 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
            "llm_load_print_meta: general.name     = Phi 3.5 Mini Instruct Bnb 4bit\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32009 '<|placeholder6|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOG token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: EOG token        = 32007 '<|end|>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2210.78 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 2048\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   156.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'general.quantization_version': '2', 'llama.attention.head_count_kv': '32', 'llama.feed_forward_length': '8192', 'llama.embedding_length': '3072', 'tokenizer.ggml.add_bos_token': 'false', 'general.size_label': 'mini', 'general.type': 'model', 'llama.context_length': '131072', 'general.name': 'Phi 3.5 Mini Instruct Bnb 4bit', 'tokenizer.ggml.bos_token_id': '1', 'general.basename': 'phi-3.5', 'tokenizer.ggml.padding_token_id': '32009', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.block_count': '32', 'llama.attention.head_count': '32', 'llama.attention.key_length': '96', 'llama.attention.value_length': '96', 'general.finetune': 'instruct-bnb-4bit', 'general.file_type': '15', 'tokenizer.ggml.pre': 'default', 'llama.vocab_size': '32064', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.rope.dimension_count': '96', 'tokenizer.ggml.model': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'general.organization': 'Unsloth', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% else %}{{'<|' + message['role'] + '|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% endif %}\"}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'user' %}{{'<|user|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% else %}{{'<|' + message['role'] + '|>\n",
            "' + message['content'] + '<|end|>\n",
            "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
            "' }}{% endif %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llm = Llama.from_pretrained(\n",
        "#     repo_id=\"winterForestStump/Phi-3.5-instruct-CBR_Bonds_info\",\n",
        "#     filename=\"*unsloth.Q4_K_M.gguf\",\n",
        "#     verbose=False\n",
        "# )"
      ],
      "metadata": {
        "id": "gyT_XbOUcwpR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(text) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    # Instruction: You have to extract securities registration numbers from the provided text.\n",
        "    # Provided text: {text}.\n",
        "    # Securities registration numbers:\n",
        "    \"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "PDaheiITdPn2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute(text):\n",
        "    prompt = generate_prompt(text)\n",
        "    output = llm(\n",
        "      prompt,\n",
        "      max_tokens=512,\n",
        "      stop = ['#', '\\n'],\n",
        "      echo=False)\n",
        "    return output"
      ],
      "metadata": {
        "id": "hEYAujE6MhIa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"\"\"\n",
        "Банк России 25.03.2024 принял решение о государственной регистрации отчета об итогах дополнительного выпуска обыкновенных акций акционерного общества «Корпорация развития жилищного строительства» (Приморский край), размещенных путем закрытой подписки, регистрационный номер дополнительного выпуска 1-01-50157-A-013D.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-bQC7oyaNNO6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execute(test)"
      ],
      "metadata": {
        "id": "bobB4sFe6W6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b39186-0638-4ca3-89b9-e98dcb83e32f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 3 prefix-match hit, remaining 159 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   31840.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   159 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   33185.86 ms /   176 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-6d00be7c-3716-434e-806c-d4fa55d3d9f3',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1732792980,\n",
              " 'model': '/content/models/unsloth.Q4_K_M.gguf',\n",
              " 'choices': [{'text': '1-01-50157-A-013D',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 162, 'completion_tokens': 18, 'total_tokens': 180}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_2 = \"\"\"\n",
        "Банк России 18.03.2024 принял решение о государственной регистрации изменений в решение о выпуске неконвертируемых процентных бездокументарных облигаций серии П001-02, размещаемых в рамках программы облигаций серии П001, общества с ограниченной ответственностью «Проспект» (г. Москва), регистрационный номер выпуска 4-02-00697-R-001P.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "d2ntv_JONWp3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execute(test_2)"
      ],
      "metadata": {
        "id": "AbdLCGrnNgLO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339e18a7-4092-4432-977e-7b033102dc53"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 35 prefix-match hit, remaining 145 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   31840.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   145 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   30895.28 ms /   162 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-42168078-6b9e-48a7-bb44-8835fe1b3400',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1732793042,\n",
              " 'model': '/content/models/unsloth.Q4_K_M.gguf',\n",
              " 'choices': [{'text': '4-02-00697-R-001P',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 180, 'completion_tokens': 18, 'total_tokens': 198}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_3 = \"\"\"\n",
        "Банк России 18.03.2024 принял решение о государственной регистрации выпусков неконвертируемых процентных бездокументарных облигаций серий З026-1-ФР и З026-2-Р, размещаемых в рамках программы облигаций серии 003Р, открытого акционерного общества «Российские железные дороги» (г. Москва). Выпускам ценных бумаг присвоены регистрационные номера 4-01-65045-D-002P и 4-02-65045-D-002P соответственно.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q82hgJAzNjDH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execute(test_3)"
      ],
      "metadata": {
        "id": "_hu1_6e1Nm-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "423e3492-8b0e-4a68-c36e-92e09b1742ba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 55 prefix-match hit, remaining 164 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   31840.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    17 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   34831.95 ms /   181 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-3eeda8d6-b6c8-4374-a60b-8457a29a2f6d',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1732793081,\n",
              " 'model': '/content/models/unsloth.Q4_K_M.gguf',\n",
              " 'choices': [{'text': '4-01-65045-D-002P',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 219, 'completion_tokens': 18, 'total_tokens': 237}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_4 = \"\"\"\n",
        "Банк России 14.03.2024 принял решение о государственной регистрации выпусков структурных процентных дисконтных неконвертируемых бездокументарных облигаций с централизованным учетом прав серий С-1-991, С-1-992, С-1-993, С-1-994, С-1-995, С-1-996, С-1-997, С-1-998, С-1-999, С-1-1000, С-1-1001, С-1-1002, С-1-1003, С-1-1004, С-1-1005, С-1-1006, С-1-1007, С-1-1008, С-1-1009, С-1-1010, размещаемых в рамках программы облигаций Банка ВТБ (публичное акционерное общество) (г. Санкт-Петербург). Выпускам ценных бумаг присвоены регистрационные номера 6-991-01000-В-001P, 6-992-01000-В-001P, 6-993-01000-В-001P, 6-994-01000-В-001P, 6-995-01000-В-001P, 6-996-01000-В-001P, 6-997-01000-В-001P, 6-998-01000-В-001P, 6-999-01000-В-001P, 6-1000-01000-В-001P, 6-1001-01000-В-001P, 6-1002-01000-В-001P, 6-1003-01000-В-001P, 6-1004-01000-В-001P, 6-1005-01000-В-001P, 6-1006-01000-В-001P, 6-1007-01000-В-001P, 6-1008-01000-В-001P, 6-1009-01000-В-001P, 6-1010-01000-В-001P соответственно.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PoVOsXxrNs5K"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execute(test_4)"
      ],
      "metadata": {
        "id": "sK06y_Y0NxYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6179d027-9ea3-4500-9978-d099cd68c254"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 36 prefix-match hit, remaining 717 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   31840.62 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   717 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  115653.32 ms /   735 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'cmpl-4a20fc75-f4f0-4dce-9320-774dc103ad0b',\n",
              " 'object': 'text_completion',\n",
              " 'created': 1732793147,\n",
              " 'model': '/content/models/unsloth.Q4_K_M.gguf',\n",
              " 'choices': [{'text': '6-991-01000-В-001P',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 753, 'completion_tokens': 19, 'total_tokens': 772}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Takeaways:\n",
        "- Good results on the short texts wwith only one security number;\n",
        "- Needs to be trained more on texts with several sec numbers"
      ],
      "metadata": {
        "id": "67WYLrQbiCXN"
      }
    }
  ]
}