{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIpdBVe6JmI5"
   },
   "source": [
    "* The Unsloth template for this notebook - https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing#scrollTo=IqM-T1RTzY6C\n",
    "\n",
    "* The Llama-3.2-3B-Instruct model - https://huggingface.co/unsloth/Llama-3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bvuy-7Lii4Si"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\" --quiet\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --quiet\n",
    "!pip install datasets mlflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1zr_0-4cv-f",
    "outputId": "a8d5eab4-7dd3-4402-aa32-f0a2c1c013c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import mlflow\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281,
     "referenced_widgets": [
      "f0fa91c571ea4e63bfd11e4cdcc2919e",
      "8579e3d68d69464aabd8e0d74270c459",
      "cc735fa539e04f2e84daae414e17ac2c",
      "1a4efbd453bb4fbe921ac37245647a7c",
      "e5577a92e19f42a1b36cf872cc1f8fda",
      "c4f3f8dfbf3a4f888eaab85637d782e8",
      "72d8a551d3c047d982cfbd631fef3505",
      "43e3028cf4de4071b5ee7ca5280a106e",
      "c8b86e4558a04834a2809da1eaa829f9",
      "1761ec483cb9417caeced55e9462daf7",
      "eb09d2e2ce824d298bd91689258112b0",
      "152bab574aab416a95f2d419d8bf9701",
      "1bd9be1cab7d42509cf0b65d87e322f3",
      "88628cecb28846dea9d933c98f12e528",
      "b4599752393a4f87ab0d6dbecfb57002",
      "ecfa9d1fcaab413698e6bcdf51b35452",
      "538c4de2193f49019f5fc7683531d47f",
      "07a2fac18394425cbb91c2d06f8394d1",
      "cbc24b157af04d658f8ac987d01b66a1",
      "d230809191eb4d8c80109c7176ba6ed1",
      "84e38aa774c244f89cdf9e686d5bde84",
      "75dc671e954b46a99a0247e794d6077b",
      "85dc6d7ae8af4c519f95fadbe1faccf1",
      "48b9a35463174f8d810f68df46f5c0b5",
      "c2b0500e3dd64b008e6c2ae9b9f5aade",
      "8d116688e9ef439db20d7cdf819df186",
      "bce9a241584748dfa41060dc805614cc",
      "2a917ec3c18249f7b2a54ad0a51fc75f",
      "cbeb4b542124492ba70c3deb2ba98944",
      "44e1a744b33c4a808390f37dd1514590",
      "6cf3c6389aa24d92a1a4bb3d8b5c3019",
      "ebd7c0590a7044f59f58027ef3963eed",
      "bdd68c3d2fcd43239bcd977320e3926d",
      "5bf92eedad8b4c3986d380c6ce1d2cbf",
      "9eb3fb4efc194bda80b2dbf9fc7c08b8",
      "3ed5c109e8014737b842b110eea03bfe",
      "af402b3c5f4d40ddad3ada3b4b521422",
      "42cfadbcb82d44c2938037162d523fe4",
      "fe317216c0994ffebf62d9faef870a5b",
      "89fd72ba06a84558b40ee0879b40af9d",
      "4867319117ab46b1862c996cd9dabd4b",
      "9b270ea7835046d796f1528835efb8cf",
      "2d4253326d974f5b971f4b09ff10b23d",
      "873945569bcc4d269bd0284c6d29d6c0",
      "ae39bcb70e1f48ae8b2740d5479d18c4",
      "b58769287b434745b329e02b4f464c23",
      "514e3539d2334e39903e04ca594742f3",
      "449a68d0d9fa418ba98931734ce9a248",
      "bb136b5591ac4e71b2080ba1fbd87a56",
      "522aaf1464a9442a91557e1c2c070fde",
      "aab868ad93cb4d4aba9b275efde2dbeb",
      "7df5f03e63fe4ac19f6aba54376eb40c",
      "18546557331f45569d71e67d0e7d9118",
      "2c0a2fe4ff0f4992a463491f94e3174d",
      "e3a8cc1f43a74be7a10e981b18c13cea"
     ]
    },
    "id": "_8HwUd4ZjCmd",
    "outputId": "7888f5d9-75e0-4bcc-d19e-a73233e366c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.10: Fast Llama patching. Transformers:4.46.2.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fa91c571ea4e63bfd11e4cdcc2919e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152bab574aab416a95f2d419d8bf9701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85dc6d7ae8af4c519f95fadbe1faccf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf92eedad8b4c3986d380c6ce1d2cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae39bcb70e1f48ae8b2740d5479d18c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = \"\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w6TSzl08jNNO",
    "outputId": "f031022f-1189-4535-c5f6-ee32d44da1cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.10 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 1994,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RXcPV2wjjyZz"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    system_message = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rYjSMCL-yPLN"
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"task\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WofTZmBDyRSW"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('winterForestStump/cbr_bonds_info_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "e4kbaC_rhqUq"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TZbeNamdzTP",
    "outputId": "467da5b3-6d6b-4e4c-c9b2-30af8a52f308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "#Instruction: You have to extract securities registration numbers from the provided text. #Provided text: Банк России 16.11.2023 принял решение о государственной регистрации выпусков неконвертируемых процентных бездокументарных облигаций серий ЗО25-Д, ЗО26-Д, ЗО27-Д, ЗО28-Д и ЗО29-Д акционерного общества «Государственная транспортная лизинговая компания» (Ямало-Ненецкий автономный округ), размещаемых путем открытой подписки. Выпускам ценных бумаг присвоены регистрационные номера 4-04-32432-H, 4-05-32432-H, 4-06-32432-H, 4-07-32432-H и 4-08-32432-H соответственно.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "4-04-32432-H, 4-05-32432-H, 4-06-32432-H, 4-07-32432-H, 4-08-32432-H<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "d36f41f8da9245d18a264022719da55b",
      "e8761b8fd7714d74862b2f6ec9ec361f",
      "3e6a69fa1dd14070ab2fe83a12c18296",
      "d4dada2cf7774e1d829339e211a5b6fa",
      "def2684b24ca41068c03bdf2713037e4",
      "55247acd1aae420abd01478f72b9781a",
      "e12a613ac3104405b722d71e4d491241",
      "641b9eabd9dc4d6d89356fe55069e83a",
      "3b2b472af55e42ce91560ce16310e2ba",
      "c1cb073d19cb49959155e1af29f21916",
      "a93a58c3e28b4e9ba5fa75b8ce8b2564",
      "9c6598625f11409797adabfa5482583a",
      "c22aa870dfe44b15a3fd67292352addd",
      "1649c2d06efa4dfa8f33a5e5c635ce60",
      "a516e5f5c6494b82b08e9aa3b758a3b2",
      "61aa8c7e29d14df8982cab38fa1543eb",
      "f0f4bb22dcaf4fa3bb0a4a3da1273db4",
      "f8cad03c09f04e6581f9016b12bfdf49",
      "a6c70205875f4755afd7a5fdb94059d1",
      "54ce3a27f6924699a597203509f92c91",
      "54eb911741d14df89ee4e4eaa336100d",
      "95aab59637594c6f9730c8a1618b5893"
     ]
    },
    "id": "KgGaZcMNw2GL",
    "outputId": "8b3f43a9-d455-4a67-d91c-f6d84b05fbd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/01 14:18:23 INFO mlflow.tracking.fluent: Experiment with name 'Llama-3.2-3B-Instruct fine-tuning' does not exist. Creating a new experiment.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36f41f8da9245d18a264022719da55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6598625f11409797adabfa5482583a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"Llama-3.2-3B-Instruct fine-tuning\"\n",
    "\n",
    "mlflow.set_experiment(name)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset = dataset['test'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs=3,\n",
    "        #max_steps = 100,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 1994,\n",
    "        evaluation_strategy = \"steps\",\n",
    "        output_dir = \"outputs\",\n",
    "        run_name=f\"{name}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n",
    "        report_to = \"mlflow\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhWUKJND90Fc",
    "outputId": "9a9970a4-a928-40a4-cdc7-49f8782f59ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "2.635 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rEQ4RNBRxPHF",
    "outputId": "2b4726f2-7237-4f01-92e7-02f0920ade20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 119 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 45\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 10:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.066700</td>\n",
       "      <td>2.302467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.176500</td>\n",
       "      <td>2.263669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.359300</td>\n",
       "      <td>2.114578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.090200</td>\n",
       "      <td>1.950215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.871600</td>\n",
       "      <td>1.787136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.706200</td>\n",
       "      <td>1.624680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.730500</td>\n",
       "      <td>1.500655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.633800</td>\n",
       "      <td>1.396631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.540800</td>\n",
       "      <td>1.302518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.251000</td>\n",
       "      <td>1.207423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.127400</td>\n",
       "      <td>1.118156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.051100</td>\n",
       "      <td>1.035978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.437200</td>\n",
       "      <td>0.962286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.409700</td>\n",
       "      <td>0.890610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.826083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.880600</td>\n",
       "      <td>0.766724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.718207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>0.673018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.962100</td>\n",
       "      <td>0.628028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.645700</td>\n",
       "      <td>0.593733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.567835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.547947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.620900</td>\n",
       "      <td>0.530979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.664700</td>\n",
       "      <td>0.513745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.498479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.713900</td>\n",
       "      <td>0.485947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.474949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.464704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.526000</td>\n",
       "      <td>0.456229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.805600</td>\n",
       "      <td>0.449914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.438200</td>\n",
       "      <td>0.444073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>0.437033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.429516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.701100</td>\n",
       "      <td>0.422416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.561500</td>\n",
       "      <td>0.415732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.327700</td>\n",
       "      <td>0.409940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.404200</td>\n",
       "      <td>0.405330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.635300</td>\n",
       "      <td>0.401667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.398495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.366100</td>\n",
       "      <td>0.395753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.393405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.391519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.668300</td>\n",
       "      <td>0.390099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.389181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.388720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96P36NfUgzga",
    "outputId": "78e8009b-81c9-48ee-bdc0-3e3284bdb662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/mlruns/ (stored 0%)\n",
      "  adding: content/mlruns/.trash/ (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/ (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/ (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/tags/ (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/tags/mlflow.user (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/tags/mlflow.source.type (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/tags/mlflow.runName (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/tags/mlflow.source.name (deflated 5%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/prediction_loss_only (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/split_batches (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/mlp_bias (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/push_to_hub_token (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataloader_prefetch_factor (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/save_strategy (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/hub_model_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/auto_find_batch_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/deepspeed (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/optim (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/disable_tqdm (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/do_predict (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/push_to_hub_organization (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/evaluation_strategy (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/hub_always_push (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/include_inputs_for_metrics (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/mp_parameters (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_packing (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/pad_token_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/tpu_num_cores (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/label_smoothing_factor (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fsdp_transformer_layer_cls_to_wrap (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/forced_eos_token_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/overwrite_output_dir (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/rope_scaling (deflated 22%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/metric_for_best_model (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/hub_private_repo (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/optim_args (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/report_to (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ddp_bucket_cap_mb (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/no_cuda (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/half_precision_backend (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/unsloth_version (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/hidden_act (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataloader_persistent_workers (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/max_length (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/debug (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/return_dict_in_generate (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/exponential_decay_length_penalty (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_cache (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ray_scope (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/log_level_replica (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/attention_dropout (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/output_dir (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/adam_beta1 (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/bf16_full_eval (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_delay (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/data_seed (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ignore_data_skip (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/tie_word_embeddings (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/decoder_start_token_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/diversity_penalty (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_cpu (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/full_determinism (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_do_concat_batches (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/rms_norm_eps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/add_cross_attention (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/logging_nan_inf_filter (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/forced_bos_token_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/cross_attention_hidden_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_of_sequences (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/gradient_checkpointing_kwargs (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/no_repeat_ngram_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/learning_rate (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ddp_find_unused_parameters (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/attention_bias (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/seed (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/is_decoder (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fp16_full_eval (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/length_penalty (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_liger (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/task_specific_params (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/encoder_no_repeat_ngram_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/tokenizer_class (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/group_by_length (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/warmup_ratio (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/temperature (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fsdp_config (deflated 33%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/return_dict (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_bfloat16 (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/max_position_embeddings (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_beam_groups (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_beams (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/architectures (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ddp_backend (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_attention_heads (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/include_tokens_per_second (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/restore_callback_states_from_checkpoint (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/chunk_size_feed_forward (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/packing (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_legacy_prediction_loop (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/top_p (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/transformers_version (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/include_num_input_tokens_seen (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataset_text_field (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/tf32 (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/do_sample (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/save_safetensors (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/logging_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/push_to_hub (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/problem_type (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/batch_eval_metrics (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/lr_scheduler_type (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/bad_words_ids (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/repetition_penalty (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/skip_memory_metrics (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/torch_compile (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/per_gpu_train_batch_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataset_batch_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/resume_from_checkpoint (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/average_tokens_across_devices (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/hidden_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/remove_unused_columns (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/label2id (deflated 18%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/torchdynamo (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/warmup_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_ipex (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/chars_per_token (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/output_hidden_states (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_use_gather_object (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_key_value_heads (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/sep_token_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/remove_invalid_values (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/torch_empty_cache_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/prefix (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_hidden_layers (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/include_for_metrics (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/begin_suppress_tokens (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/tie_encoder_decoder (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_return_sequences (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/early_stopping (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/log_on_each_node (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/max_grad_norm (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/per_device_eval_batch_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/push_to_hub_model_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/initializer_range (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/quantization_config (deflated 44%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/torchscript (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_mps_device (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/log_level (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/model_type (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/do_train (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/gradient_accumulation_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/weight_decay (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/_name_or_path (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/do_eval (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_on_start (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/label_names (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/gradient_checkpointing (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/torch_compile_backend (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/load_best_model_at_end (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/typical_p (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/logging_strategy (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/id2label (deflated 21%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/length_column_name (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/save_total_limit (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/num_train_epochs (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/intermediate_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eos_token_id (deflated 29%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/adam_epsilon (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/tpu_metrics_debug (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/pruned_heads (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/output_attentions (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/is_encoder_decoder (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/save_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/head_dim (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/save_on_each_node (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataloader_pin_memory (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/lr_scheduler_kwargs (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/per_gpu_eval_batch_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/per_device_train_batch_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataloader_drop_last (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fp16_opt_level (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/rope_theta (deflated 13%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/suppress_tokens (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/optim_target_modules (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/bos_token_id (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataset_num_proc (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/_attn_implementation_autoset (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/output_scores (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_accumulation_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/hub_token (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/pretraining_tp (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/logging_first_step (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/torch_dtype (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fsdp (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/model_init_kwargs (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/jit_mode_eval (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/torch_compile_mode (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/accelerator_config (deflated 33%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataloader_num_workers (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ddp_timeout (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/vocab_size (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fp16 (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/hub_strategy (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/eval_strategy (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/use_liger_kernel (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/neftune_noise_alpha (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/save_only_model (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/top_k (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/max_seq_length (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/max_steps (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/past_index (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/finetuning_task (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/adam_beta2 (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/bf16 (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fsdp_min_num_params (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dataset_kwargs (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/ddp_broadcast_buffers (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/local_rank (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/tf_legacy_loss (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/min_length (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/logging_dir (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/greater_is_better (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/adafactor (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/dispatch_batches (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/fp16_backend (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/params/run_name (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/ (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/train_samples_per_second (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/eval_runtime (deflated 62%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/train_steps_per_second (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/learning_rate (deflated 67%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/eval_samples_per_second (deflated 64%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/loss (deflated 61%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/eval_loss (deflated 56%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/train_loss (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/epoch (deflated 78%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/total_flos (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/train_runtime (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/eval_steps_per_second (deflated 68%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/metrics/grad_norm (deflated 56%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/meta.yaml (deflated 43%)\n",
      "  adding: content/mlruns/111638724200738015/2d4b828fbc6d43d595d0e6c7bb2a64f3/artifacts/ (stored 0%)\n",
      "  adding: content/mlruns/111638724200738015/meta.yaml (deflated 29%)\n",
      "  adding: content/mlruns/0/ (stored 0%)\n",
      "  adding: content/mlruns/0/meta.yaml (deflated 25%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r mlruns.zip /content/mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "SRZ3e4gwg11R",
    "outputId": "d7dea55e-2b87-4a93-9255-0ae0c6714b52"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_917eae45-1708-4db0-8cf8-50a517788a15\", \"mlruns.zip\", 81488)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"mlruns.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTZixzlvxSj2",
    "outputId": "5b2d5ce5-e0c2-4cda-9a19-e4a9564995b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676.5596 seconds used for training.\n",
      "11.28 minutes used for training.\n",
      "Peak reserved memory = 6.357 GB.\n",
      "Peak reserved memory for training = 3.722 GB.\n",
      "Peak reserved memory % of max memory = 43.104 %.\n",
      "Peak reserved memory for training % of max memory = 25.237 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXBVfutW-U_w"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token='') # token = \"hf_...\"\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = '') # token = \"hf_...\"\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = '') # token = \"hf_...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8c1b964414004490b45b94ccbd534c68",
      "c0c1a4a9e41f4233a8136692c78efec8",
      "96758555a2144d959c19339560bb33fa",
      "d8a6f03b2f1844078fe6cc478e332d30",
      "560bd6457cca4e05a9a4a4981f4d0244",
      "2d140a6473894e6ea0ad782e3df1ed8f",
      "ac77907d7003478aa8c1b27692c0fd3e",
      "1c66b99b96fe452f8e4aa35187399a27",
      "a0dd32f105e047c6a3138774e2b6c624",
      "184f3ae2ab164f98a8127bbc006a9914",
      "b037357d8b2c449e9264da2182f648c2",
      "ae4b979db605436e9b02c38a936d4b35",
      "fd38924f37ed41a99e4dd3abbd0ec5fe",
      "a33f1ddb35544c149a41042169d8b30d",
      "98e83bbfcfe54123b29558827800f787",
      "e8f032504a1c4486886a5fa6f664456d",
      "777365bb32e74dabbd89a5c742baf655",
      "fad2890028de47ae9c2fad3536b29a60",
      "79b10dab6ea34dcbb0ae45700c3b5f74",
      "54cbbc81712e473a9c2c4f12000761fb",
      "2ac1edb35926418fa674fa73c0f90253",
      "ebf81212466c471e97ed122c7e3982f3",
      "17b9ed71a5814b78b2cfa0ca9c3f393b",
      "2e5c8f4993244e298a3ba12aa624472b",
      "60551b540eb846ab85abaefa47e44b04",
      "df71b1a065c94175bf082cae3ec8d373",
      "1f8846e2d8464ee0b9b3af9b22975c83",
      "d96eb7b637864c668f5a80a7bed5e03c",
      "398132cb77224082b2b05bc46101d3c5",
      "86886b2743a344c480bb6fa5cdcdc585",
      "422948d93e8e44a890a5097e6fad079d",
      "aa9684fa2c674bd9927e61e571fe5ba5",
      "c2d55b7b9fbd4d68ba70d37334651f03",
      "eff884c9bd074be289e3b96482ba6178",
      "870fee3934c94a1fa14118f2a6de3a43",
      "2f42a3127e13484b8e72609cdbde1e0e",
      "1cc281d1c78f46b6aadfa995dbf6954c",
      "4c686388fe7d4980a67872c3898735f1",
      "2f092c19db9b40e1943b45acc0351b62",
      "d301f908f6634bd6a6373b4d8c7107ea",
      "4cce8f7fda7443d79f36b349983e2cb8",
      "775c048a3fd04a0799dfb522d242d72a",
      "4262b2f39a084efc9bce023b081e2d01",
      "6d5eeb40d7694a82a10696c89c926728",
      "e3b1063d0458489880ed9915e4181b2d",
      "29bd5ecd4703431baf0b0a80b9aa75c2",
      "bc0983a6969245f396e7db840eadfbbf",
      "f9aa8df936344c8eb26ae91cd6dcc216",
      "8a179ef075db4d14b56e5ed1e1cc2ccb",
      "0ce793eec15044768be887839e7f331d",
      "35f3292c3c55498da28e67c3eee4b338",
      "33a55f4d140d4b3e8b5ff6e7a7324fab",
      "7dfa290dd97448f48ea155ef601f67e9",
      "81f19af194ee41fd800e9f3b09fe4893",
      "eda772e31d944aa0bf01ce1d6ae10873",
      "fe6529b00e01421fb0bb9526d05f48ae",
      "0ec43093ace44fe3aa876e04702f942b",
      "a9b3690d3b2147ffaabccca83b19641a",
      "f79ce6c38bcd494b9bd0dcd36d083af6",
      "bae527f332204906ac50d3d0844922db",
      "7b602d8b5aca401f95ce0dd1915c31d5",
      "29740ce3e6c040c188123ed97d5f9cac",
      "a1a9cbf2d0294ef6a7bb13b276257d14",
      "e90f0b5c678b40ecb9a92c10f4741cb7",
      "9c0efb27d48247e8966b8aa0b6f5fb1b",
      "054063f9a79a4e7c844bc32ae64e41d8"
     ]
    },
    "id": "8BTWoAWI-5BZ",
    "outputId": "95390e81-83e1-4ad0-8ecd-3796dc5fae50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
      "We shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\n",
      "To force `safe_serialization`, set it to `None` instead.\n",
      "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
      "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
      "Unsloth: Will remove a cached repo with size 2.2G\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 5.15 out of 12.67 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:02<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Unsloth: Saving winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/pytorch_model-00001-of-00002.bin...\n",
      "Unsloth: Saving winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/pytorch_model-00002-of-00002.bin...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info into f16 GGUF format.\n",
      "The output location will be /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: Llama-3.2-3B-Instruct-CBR_Bonds_info\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 24\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 July 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content'] %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\n",
      "\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\n",
      "\n",
      "\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\n",
      "\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\n",
      "\n",
      "\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content'] %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\n",
      "\n",
      "\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\n",
      "\n",
      "\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\n",
      "\n",
      "\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
      "Writing: 100%|██████████| 6.43G/6.43G [01:36<00:00, 66.6Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf\n",
      "Unsloth: Conversion completed! Output location: /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 4233 (86dc11c5)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf' to '/content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  1918.35 MB\n",
      "\n",
      "main: quantize time = 352437.35 ms\n",
      "main:    total time = 352437.35 ms\n",
      "Unsloth: Conversion completed! Output location: /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.Q4_K_M.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\n",
      "main: build = 4233 (86dc11c5)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf' to '/content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.Q8_0.gguf' as Q8_0 using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q8_0 .. size =   751.50 MiB ->   399.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    18.00 MiB ->     9.56 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     6.00 MiB ->     3.19 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q8_0 .. size =    48.00 MiB ->    25.50 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  3255.90 MB\n",
      "\n",
      "main: quantize time = 82825.33 ms\n",
      "main:    total time = 82825.33 ms\n",
      "Unsloth: Conversion completed! Output location: /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.Q8_0.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\n",
      "main: build = 4233 (86dc11c5)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf' to '/content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.Q5_K_M.gguf' as Q5_K_M using 4 threads\n",
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3b Instruct Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = llama-3.2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type  f16:  197 tensors\n",
      "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
      "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
      "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q5_K .. size =     6.00 MiB ->     2.06 MiB\n",
      "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q5_K .. size =    18.00 MiB ->     6.19 MiB\n",
      "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
      "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
      "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q5_K .. size =    48.00 MiB ->    16.50 MiB\n",
      "llama_model_quantize_internal: model size  =  6128.17 MB\n",
      "llama_model_quantize_internal: quant size  =  2207.10 MB\n",
      "\n",
      "main: quantize time = 286492.94 ms\n",
      "main:    total time = 286492.94 ms\n",
      "Unsloth: Conversion completed! Output location: /content/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/unsloth.Q5_K_M.gguf\n",
      "Unsloth: Saved Ollama Modelfile to winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info/Modelfile\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1b964414004490b45b94ccbd534c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4b979db605436e9b02c38a936d4b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b9ed71a5814b78b2cfa0ca9c3f393b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff884c9bd074be289e3b96482ba6178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q8_0.gguf:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b1063d0458489880ed9915e4181b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6529b00e01421fb0bb9526d05f48ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.Q5_K_M.gguf:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Ollama Modelfile to https://huggingface.co/winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info\n"
     ]
    }
   ],
   "source": [
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if True:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"winterForestStump/Llama-3.2-3B-Instruct-CBR_Bonds_info\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "054063f9a79a4e7c844bc32ae64e41d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07a2fac18394425cbb91c2d06f8394d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0ce793eec15044768be887839e7f331d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ec43093ace44fe3aa876e04702f942b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b602d8b5aca401f95ce0dd1915c31d5",
      "placeholder": "​",
      "style": "IPY_MODEL_29740ce3e6c040c188123ed97d5f9cac",
      "value": "unsloth.Q5_K_M.gguf: "
     }
    },
    "152bab574aab416a95f2d419d8bf9701": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1bd9be1cab7d42509cf0b65d87e322f3",
       "IPY_MODEL_88628cecb28846dea9d933c98f12e528",
       "IPY_MODEL_b4599752393a4f87ab0d6dbecfb57002"
      ],
      "layout": "IPY_MODEL_ecfa9d1fcaab413698e6bcdf51b35452"
     }
    },
    "1649c2d06efa4dfa8f33a5e5c635ce60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6c70205875f4755afd7a5fdb94059d1",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54ce3a27f6924699a597203509f92c91",
      "value": 30
     }
    },
    "1761ec483cb9417caeced55e9462daf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17b9ed71a5814b78b2cfa0ca9c3f393b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e5c8f4993244e298a3ba12aa624472b",
       "IPY_MODEL_60551b540eb846ab85abaefa47e44b04",
       "IPY_MODEL_df71b1a065c94175bf082cae3ec8d373"
      ],
      "layout": "IPY_MODEL_1f8846e2d8464ee0b9b3af9b22975c83"
     }
    },
    "184f3ae2ab164f98a8127bbc006a9914": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18546557331f45569d71e67d0e7d9118": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a4efbd453bb4fbe921ac37245647a7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1761ec483cb9417caeced55e9462daf7",
      "placeholder": "​",
      "style": "IPY_MODEL_eb09d2e2ce824d298bd91689258112b0",
      "value": " 2.24G/2.24G [00:20&lt;00:00, 324MB/s]"
     }
    },
    "1bd9be1cab7d42509cf0b65d87e322f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_538c4de2193f49019f5fc7683531d47f",
      "placeholder": "​",
      "style": "IPY_MODEL_07a2fac18394425cbb91c2d06f8394d1",
      "value": "generation_config.json: 100%"
     }
    },
    "1c66b99b96fe452f8e4aa35187399a27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cc281d1c78f46b6aadfa995dbf6954c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4262b2f39a084efc9bce023b081e2d01",
      "placeholder": "​",
      "style": "IPY_MODEL_6d5eeb40d7694a82a10696c89c926728",
      "value": " 3.42G/? [00:29&lt;00:00, 912MB/s]"
     }
    },
    "1f8846e2d8464ee0b9b3af9b22975c83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29740ce3e6c040c188123ed97d5f9cac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29bd5ecd4703431baf0b0a80b9aa75c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ce793eec15044768be887839e7f331d",
      "placeholder": "​",
      "style": "IPY_MODEL_35f3292c3c55498da28e67c3eee4b338",
      "value": "100%"
     }
    },
    "2a917ec3c18249f7b2a54ad0a51fc75f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ac1edb35926418fa674fa73c0f90253": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c0a2fe4ff0f4992a463491f94e3174d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d140a6473894e6ea0ad782e3df1ed8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d4253326d974f5b971f4b09ff10b23d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e5c8f4993244e298a3ba12aa624472b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d96eb7b637864c668f5a80a7bed5e03c",
      "placeholder": "​",
      "style": "IPY_MODEL_398132cb77224082b2b05bc46101d3c5",
      "value": "100%"
     }
    },
    "2f092c19db9b40e1943b45acc0351b62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f42a3127e13484b8e72609cdbde1e0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cce8f7fda7443d79f36b349983e2cb8",
      "max": 3421899584,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_775c048a3fd04a0799dfb522d242d72a",
      "value": 3421899584
     }
    },
    "33a55f4d140d4b3e8b5ff6e7a7324fab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35f3292c3c55498da28e67c3eee4b338": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "398132cb77224082b2b05bc46101d3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b2b472af55e42ce91560ce16310e2ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e6a69fa1dd14070ab2fe83a12c18296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_641b9eabd9dc4d6d89356fe55069e83a",
      "max": 119,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b2b472af55e42ce91560ce16310e2ba",
      "value": 119
     }
    },
    "3ed5c109e8014737b842b110eea03bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4867319117ab46b1862c996cd9dabd4b",
      "max": 9085657,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b270ea7835046d796f1528835efb8cf",
      "value": 9085657
     }
    },
    "422948d93e8e44a890a5097e6fad079d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4262b2f39a084efc9bce023b081e2d01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42cfadbcb82d44c2938037162d523fe4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43e3028cf4de4071b5ee7ca5280a106e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "449a68d0d9fa418ba98931734ce9a248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c0a2fe4ff0f4992a463491f94e3174d",
      "placeholder": "​",
      "style": "IPY_MODEL_e3a8cc1f43a74be7a10e981b18c13cea",
      "value": " 454/454 [00:00&lt;00:00, 7.55kB/s]"
     }
    },
    "44e1a744b33c4a808390f37dd1514590": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4867319117ab46b1862c996cd9dabd4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48b9a35463174f8d810f68df46f5c0b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a917ec3c18249f7b2a54ad0a51fc75f",
      "placeholder": "​",
      "style": "IPY_MODEL_cbeb4b542124492ba70c3deb2ba98944",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "4c686388fe7d4980a67872c3898735f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cce8f7fda7443d79f36b349983e2cb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514e3539d2334e39903e04ca594742f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7df5f03e63fe4ac19f6aba54376eb40c",
      "max": 454,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_18546557331f45569d71e67d0e7d9118",
      "value": 454
     }
    },
    "522aaf1464a9442a91557e1c2c070fde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "538c4de2193f49019f5fc7683531d47f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54cbbc81712e473a9c2c4f12000761fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54ce3a27f6924699a597203509f92c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54eb911741d14df89ee4e4eaa336100d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55247acd1aae420abd01478f72b9781a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "560bd6457cca4e05a9a4a4981f4d0244": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bf92eedad8b4c3986d380c6ce1d2cbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9eb3fb4efc194bda80b2dbf9fc7c08b8",
       "IPY_MODEL_3ed5c109e8014737b842b110eea03bfe",
       "IPY_MODEL_af402b3c5f4d40ddad3ada3b4b521422"
      ],
      "layout": "IPY_MODEL_42cfadbcb82d44c2938037162d523fe4"
     }
    },
    "60551b540eb846ab85abaefa47e44b04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86886b2743a344c480bb6fa5cdcdc585",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_422948d93e8e44a890a5097e6fad079d",
      "value": 1
     }
    },
    "61aa8c7e29d14df8982cab38fa1543eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "641b9eabd9dc4d6d89356fe55069e83a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cf3c6389aa24d92a1a4bb3d8b5c3019": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6d5eeb40d7694a82a10696c89c926728": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72d8a551d3c047d982cfbd631fef3505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75dc671e954b46a99a0247e794d6077b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "775c048a3fd04a0799dfb522d242d72a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "777365bb32e74dabbd89a5c742baf655": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79b10dab6ea34dcbb0ae45700c3b5f74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b602d8b5aca401f95ce0dd1915c31d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7df5f03e63fe4ac19f6aba54376eb40c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dfa290dd97448f48ea155ef601f67e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "81f19af194ee41fd800e9f3b09fe4893": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84e38aa774c244f89cdf9e686d5bde84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8579e3d68d69464aabd8e0d74270c459": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4f3f8dfbf3a4f888eaab85637d782e8",
      "placeholder": "​",
      "style": "IPY_MODEL_72d8a551d3c047d982cfbd631fef3505",
      "value": "model.safetensors: 100%"
     }
    },
    "85dc6d7ae8af4c519f95fadbe1faccf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48b9a35463174f8d810f68df46f5c0b5",
       "IPY_MODEL_c2b0500e3dd64b008e6c2ae9b9f5aade",
       "IPY_MODEL_8d116688e9ef439db20d7cdf819df186"
      ],
      "layout": "IPY_MODEL_bce9a241584748dfa41060dc805614cc"
     }
    },
    "86886b2743a344c480bb6fa5cdcdc585": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "870fee3934c94a1fa14118f2a6de3a43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f092c19db9b40e1943b45acc0351b62",
      "placeholder": "​",
      "style": "IPY_MODEL_d301f908f6634bd6a6373b4d8c7107ea",
      "value": "unsloth.Q8_0.gguf: "
     }
    },
    "873945569bcc4d269bd0284c6d29d6c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88628cecb28846dea9d933c98f12e528": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbc24b157af04d658f8ac987d01b66a1",
      "max": 184,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d230809191eb4d8c80109c7176ba6ed1",
      "value": 184
     }
    },
    "89fd72ba06a84558b40ee0879b40af9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a179ef075db4d14b56e5ed1e1cc2ccb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c1b964414004490b45b94ccbd534c68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c0c1a4a9e41f4233a8136692c78efec8",
       "IPY_MODEL_96758555a2144d959c19339560bb33fa",
       "IPY_MODEL_d8a6f03b2f1844078fe6cc478e332d30"
      ],
      "layout": "IPY_MODEL_560bd6457cca4e05a9a4a4981f4d0244"
     }
    },
    "8d116688e9ef439db20d7cdf819df186": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebd7c0590a7044f59f58027ef3963eed",
      "placeholder": "​",
      "style": "IPY_MODEL_bdd68c3d2fcd43239bcd977320e3926d",
      "value": " 54.6k/54.6k [00:00&lt;00:00, 703kB/s]"
     }
    },
    "95aab59637594c6f9730c8a1618b5893": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96758555a2144d959c19339560bb33fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c66b99b96fe452f8e4aa35187399a27",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0dd32f105e047c6a3138774e2b6c624",
      "value": 1
     }
    },
    "98e83bbfcfe54123b29558827800f787": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ac1edb35926418fa674fa73c0f90253",
      "placeholder": "​",
      "style": "IPY_MODEL_ebf81212466c471e97ed122c7e3982f3",
      "value": " 2.03G/? [00:18&lt;00:00, 904MB/s]"
     }
    },
    "9b270ea7835046d796f1528835efb8cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c0efb27d48247e8966b8aa0b6f5fb1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c6598625f11409797adabfa5482583a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c22aa870dfe44b15a3fd67292352addd",
       "IPY_MODEL_1649c2d06efa4dfa8f33a5e5c635ce60",
       "IPY_MODEL_a516e5f5c6494b82b08e9aa3b758a3b2"
      ],
      "layout": "IPY_MODEL_61aa8c7e29d14df8982cab38fa1543eb"
     }
    },
    "9eb3fb4efc194bda80b2dbf9fc7c08b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe317216c0994ffebf62d9faef870a5b",
      "placeholder": "​",
      "style": "IPY_MODEL_89fd72ba06a84558b40ee0879b40af9d",
      "value": "tokenizer.json: 100%"
     }
    },
    "a0dd32f105e047c6a3138774e2b6c624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a1a9cbf2d0294ef6a7bb13b276257d14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a33f1ddb35544c149a41042169d8b30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_79b10dab6ea34dcbb0ae45700c3b5f74",
      "max": 2019377984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_54cbbc81712e473a9c2c4f12000761fb",
      "value": 2019377984
     }
    },
    "a516e5f5c6494b82b08e9aa3b758a3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54eb911741d14df89ee4e4eaa336100d",
      "placeholder": "​",
      "style": "IPY_MODEL_95aab59637594c6f9730c8a1618b5893",
      "value": " 30/30 [00:03&lt;00:00, 10.76 examples/s]"
     }
    },
    "a6c70205875f4755afd7a5fdb94059d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a93a58c3e28b4e9ba5fa75b8ce8b2564": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9b3690d3b2147ffaabccca83b19641a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1a9cbf2d0294ef6a7bb13b276257d14",
      "max": 2322154304,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e90f0b5c678b40ecb9a92c10f4741cb7",
      "value": 2322154304
     }
    },
    "aa9684fa2c674bd9927e61e571fe5ba5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aab868ad93cb4d4aba9b275efde2dbeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac77907d7003478aa8c1b27692c0fd3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae39bcb70e1f48ae8b2740d5479d18c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b58769287b434745b329e02b4f464c23",
       "IPY_MODEL_514e3539d2334e39903e04ca594742f3",
       "IPY_MODEL_449a68d0d9fa418ba98931734ce9a248"
      ],
      "layout": "IPY_MODEL_bb136b5591ac4e71b2080ba1fbd87a56"
     }
    },
    "ae4b979db605436e9b02c38a936d4b35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fd38924f37ed41a99e4dd3abbd0ec5fe",
       "IPY_MODEL_a33f1ddb35544c149a41042169d8b30d",
       "IPY_MODEL_98e83bbfcfe54123b29558827800f787"
      ],
      "layout": "IPY_MODEL_e8f032504a1c4486886a5fa6f664456d"
     }
    },
    "af402b3c5f4d40ddad3ada3b4b521422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d4253326d974f5b971f4b09ff10b23d",
      "placeholder": "​",
      "style": "IPY_MODEL_873945569bcc4d269bd0284c6d29d6c0",
      "value": " 9.09M/9.09M [00:00&lt;00:00, 27.3MB/s]"
     }
    },
    "b037357d8b2c449e9264da2182f648c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4599752393a4f87ab0d6dbecfb57002": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84e38aa774c244f89cdf9e686d5bde84",
      "placeholder": "​",
      "style": "IPY_MODEL_75dc671e954b46a99a0247e794d6077b",
      "value": " 184/184 [00:00&lt;00:00, 3.24kB/s]"
     }
    },
    "b58769287b434745b329e02b4f464c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_522aaf1464a9442a91557e1c2c070fde",
      "placeholder": "​",
      "style": "IPY_MODEL_aab868ad93cb4d4aba9b275efde2dbeb",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "bae527f332204906ac50d3d0844922db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb136b5591ac4e71b2080ba1fbd87a56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc0983a6969245f396e7db840eadfbbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33a55f4d140d4b3e8b5ff6e7a7324fab",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7dfa290dd97448f48ea155ef601f67e9",
      "value": 1
     }
    },
    "bce9a241584748dfa41060dc805614cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdd68c3d2fcd43239bcd977320e3926d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0c1a4a9e41f4233a8136692c78efec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d140a6473894e6ea0ad782e3df1ed8f",
      "placeholder": "​",
      "style": "IPY_MODEL_ac77907d7003478aa8c1b27692c0fd3e",
      "value": "100%"
     }
    },
    "c1cb073d19cb49959155e1af29f21916": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c22aa870dfe44b15a3fd67292352addd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0f4bb22dcaf4fa3bb0a4a3da1273db4",
      "placeholder": "​",
      "style": "IPY_MODEL_f8cad03c09f04e6581f9016b12bfdf49",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "c2b0500e3dd64b008e6c2ae9b9f5aade": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44e1a744b33c4a808390f37dd1514590",
      "max": 54598,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6cf3c6389aa24d92a1a4bb3d8b5c3019",
      "value": 54598
     }
    },
    "c2d55b7b9fbd4d68ba70d37334651f03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4f3f8dfbf3a4f888eaab85637d782e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8b86e4558a04834a2809da1eaa829f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cbc24b157af04d658f8ac987d01b66a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbeb4b542124492ba70c3deb2ba98944": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc735fa539e04f2e84daae414e17ac2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43e3028cf4de4071b5ee7ca5280a106e",
      "max": 2242762780,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c8b86e4558a04834a2809da1eaa829f9",
      "value": 2242762567
     }
    },
    "d230809191eb4d8c80109c7176ba6ed1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d301f908f6634bd6a6373b4d8c7107ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d36f41f8da9245d18a264022719da55b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e8761b8fd7714d74862b2f6ec9ec361f",
       "IPY_MODEL_3e6a69fa1dd14070ab2fe83a12c18296",
       "IPY_MODEL_d4dada2cf7774e1d829339e211a5b6fa"
      ],
      "layout": "IPY_MODEL_def2684b24ca41068c03bdf2713037e4"
     }
    },
    "d4dada2cf7774e1d829339e211a5b6fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1cb073d19cb49959155e1af29f21916",
      "placeholder": "​",
      "style": "IPY_MODEL_a93a58c3e28b4e9ba5fa75b8ce8b2564",
      "value": " 119/119 [00:03&lt;00:00, 38.89 examples/s]"
     }
    },
    "d8a6f03b2f1844078fe6cc478e332d30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_184f3ae2ab164f98a8127bbc006a9914",
      "placeholder": "​",
      "style": "IPY_MODEL_b037357d8b2c449e9264da2182f648c2",
      "value": " 1/1 [00:18&lt;00:00, 18.50s/it]"
     }
    },
    "d96eb7b637864c668f5a80a7bed5e03c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "def2684b24ca41068c03bdf2713037e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df71b1a065c94175bf082cae3ec8d373": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aa9684fa2c674bd9927e61e571fe5ba5",
      "placeholder": "​",
      "style": "IPY_MODEL_c2d55b7b9fbd4d68ba70d37334651f03",
      "value": " 1/1 [00:30&lt;00:00, 30.20s/it]"
     }
    },
    "e12a613ac3104405b722d71e4d491241": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3a8cc1f43a74be7a10e981b18c13cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3b1063d0458489880ed9915e4181b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_29bd5ecd4703431baf0b0a80b9aa75c2",
       "IPY_MODEL_bc0983a6969245f396e7db840eadfbbf",
       "IPY_MODEL_f9aa8df936344c8eb26ae91cd6dcc216"
      ],
      "layout": "IPY_MODEL_8a179ef075db4d14b56e5ed1e1cc2ccb"
     }
    },
    "e5577a92e19f42a1b36cf872cc1f8fda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8761b8fd7714d74862b2f6ec9ec361f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55247acd1aae420abd01478f72b9781a",
      "placeholder": "​",
      "style": "IPY_MODEL_e12a613ac3104405b722d71e4d491241",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "e8f032504a1c4486886a5fa6f664456d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e90f0b5c678b40ecb9a92c10f4741cb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb09d2e2ce824d298bd91689258112b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebd7c0590a7044f59f58027ef3963eed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebf81212466c471e97ed122c7e3982f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ecfa9d1fcaab413698e6bcdf51b35452": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eda772e31d944aa0bf01ce1d6ae10873": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eff884c9bd074be289e3b96482ba6178": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_870fee3934c94a1fa14118f2a6de3a43",
       "IPY_MODEL_2f42a3127e13484b8e72609cdbde1e0e",
       "IPY_MODEL_1cc281d1c78f46b6aadfa995dbf6954c"
      ],
      "layout": "IPY_MODEL_4c686388fe7d4980a67872c3898735f1"
     }
    },
    "f0f4bb22dcaf4fa3bb0a4a3da1273db4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0fa91c571ea4e63bfd11e4cdcc2919e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8579e3d68d69464aabd8e0d74270c459",
       "IPY_MODEL_cc735fa539e04f2e84daae414e17ac2c",
       "IPY_MODEL_1a4efbd453bb4fbe921ac37245647a7c"
      ],
      "layout": "IPY_MODEL_e5577a92e19f42a1b36cf872cc1f8fda"
     }
    },
    "f79ce6c38bcd494b9bd0dcd36d083af6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c0efb27d48247e8966b8aa0b6f5fb1b",
      "placeholder": "​",
      "style": "IPY_MODEL_054063f9a79a4e7c844bc32ae64e41d8",
      "value": " 2.34G/? [00:20&lt;00:00, 348MB/s]"
     }
    },
    "f8cad03c09f04e6581f9016b12bfdf49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9aa8df936344c8eb26ae91cd6dcc216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81f19af194ee41fd800e9f3b09fe4893",
      "placeholder": "​",
      "style": "IPY_MODEL_eda772e31d944aa0bf01ce1d6ae10873",
      "value": " 1/1 [00:20&lt;00:00, 20.74s/it]"
     }
    },
    "fad2890028de47ae9c2fad3536b29a60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd38924f37ed41a99e4dd3abbd0ec5fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_777365bb32e74dabbd89a5c742baf655",
      "placeholder": "​",
      "style": "IPY_MODEL_fad2890028de47ae9c2fad3536b29a60",
      "value": "unsloth.Q4_K_M.gguf: "
     }
    },
    "fe317216c0994ffebf62d9faef870a5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe6529b00e01421fb0bb9526d05f48ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ec43093ace44fe3aa876e04702f942b",
       "IPY_MODEL_a9b3690d3b2147ffaabccca83b19641a",
       "IPY_MODEL_f79ce6c38bcd494b9bd0dcd36d083af6"
      ],
      "layout": "IPY_MODEL_bae527f332204906ac50d3d0844922db"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
